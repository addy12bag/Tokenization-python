# ğŸ§¾ Verdict Dataset Tokenization

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)
![NLP](https://img.shields.io/badge/NLP-Tokenization-orange?logo=spaCy)
![License](https://img.shields.io/badge/License-MIT-green)

A clean and simple pipeline for **tokenizing legal verdict texts** using powerful NLP tools like **NLTK**, **spaCy**, and **Hugging Face Transformers**.  
Ideal for preprocessing verdict data for legal NLP tasks such as classification, summarization, or sentiment analysis.

---

## ğŸ“‚ Project Structure


---

## ğŸ“Œ Features

- âœ… Read and clean text data from `.txt` file
- âœ‚ï¸ Tokenize each verdict using:
  - **NLTK** â€“ basic word-level tokenizer
  - **spaCy** â€“ advanced tokenizer with linguistic features
  - **Hugging Face Transformers** â€“ token IDs for transformer models
- ğŸ’¾ Save or use tokenized data in downstream NLP tasks

---

## ğŸ“„ Sample Text File (`verdicts.txt`)

Each line represents a verdict entry:


---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/addy12bag/Tokenization-python.git
cd verdict-tokenization
