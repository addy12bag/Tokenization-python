# 🧾 Verdict Dataset Tokenization

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python)
![NLP](https://img.shields.io/badge/NLP-Tokenization-orange?logo=spaCy)
![License](https://img.shields.io/badge/License-MIT-green)

A clean and simple pipeline for **tokenizing legal verdict texts** using powerful NLP tools like **NLTK**, **spaCy**, and **Hugging Face Transformers**.  
Ideal for preprocessing verdict data for legal NLP tasks such as classification, summarization, or sentiment analysis.

---

## 📂 Project Structure


---

## 📌 Features

- ✅ Read and clean text data from `.txt` file
- ✂️ Tokenize each verdict using:
  - **NLTK** – basic word-level tokenizer
  - **spaCy** – advanced tokenizer with linguistic features
  - **Hugging Face Transformers** – token IDs for transformer models
- 💾 Save or use tokenized data in downstream NLP tasks

---

## 📄 Sample Text File (`verdicts.txt`)

Each line represents a verdict entry:


---

## 🚀 Quick Start

### 1️⃣ Clone the Repository

```bash
git clone https://github.com/addy12bag/Tokenization-python.git
cd verdict-tokenization
